# Project NyayAI

**A 103M Parameter LLM Trained from Scratch on Indian Legal Data**

NyayAI is a custom-built GPT-style language model trained entirely from scratch on 269 million tokens of Indian Supreme Court and High Court judgments. No pre-trained weights, no fine-tuning â€” every parameter was learned from raw legal text.

---

## ğŸ¯ The Problem

India's legal system faces a staggering backlog of over **5 crore pending cases**. This judicial pendency causes inordinate delays, denying timely justice to millions. Legal research, case analysis, and document drafting remain extremely time-intensive bottlenecks.

## ğŸ’¡ The Solution

NyayAI is a foundational step toward AI-assisted legal intelligence. It's a **specialist model** â€” built from the ground up to understand and generate text in the language and structure of Indian law.

This repository contains the **complete, end-to-end pipeline**: raw data processing â†’ tokenization â†’ model architecture â†’ distributed GPU training â†’ local inference â†’ web UI.

---

## âœ¨ Key Highlights

- **Built from scratch** â€” Custom GPT architecture implemented in PyTorch, no HuggingFace dependencies
- **103M parameters** â€” 9-layer transformer with 12 attention heads and 768-dim embeddings
- **269M training tokens** â€” 1.25 GB of cleaned Indian legal judgments (Supreme Court + High Courts)
- **Weight tying** â€” Token embedding weights shared with output head, reducing parameter count
- **Cosine LR schedule** â€” Warmup + cosine decay for stable training
- **Fault-tolerant training** â€” Per-epoch checkpointing with auto-download and resumable training
- **Runs locally** â€” Fast CPU inference (~10 tokens/sec), no GPU needed for generation
- **Web interface** â€” Dark-themed premium UI with generation controls

---

## ğŸ§  Model Architecture

```
Input Tokens
    â†“
Token Embedding (50,257 Ã— 768) + Positional Embedding (512 Ã— 768)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Block (Ã—9)              â”‚
â”‚  â”œâ”€â”€ LayerNorm                       â”‚
â”‚  â”œâ”€â”€ Multi-Head Attention (12 heads) â”‚
â”‚  â”‚   â”œâ”€â”€ Q, K, V projections (768)   â”‚
â”‚  â”‚   â”œâ”€â”€ Causal mask                 â”‚
â”‚  â”‚   â””â”€â”€ Output projection          â”‚
â”‚  â”œâ”€â”€ Residual connection + Dropout   â”‚
â”‚  â”œâ”€â”€ LayerNorm                       â”‚
â”‚  â”œâ”€â”€ Feed-Forward (768 â†’ 3072 â†’ 768) â”‚
â”‚  â””â”€â”€ Residual connection + Dropout   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final LayerNorm
    â†“
Output Head (weight-tied with Token Embedding)
    â†“
Logits (50,257)
```

| Parameter           | Value                           |
| ------------------- | ------------------------------- |
| Vocabulary Size     | 50,257 (GPT-2 BPE via tiktoken) |
| Context Length      | 512 tokens                      |
| Embedding Dimension | 768                             |
| Attention Heads     | 12 (head dim = 64)              |
| Transformer Layers  | 9                               |
| Feed-Forward Hidden | 3,072 (4Ã— emb_dim)              |
| Dropout Rate        | 0.1                             |
| Total Parameters    | **102,762,240 (~103M)**         |
| Model Size (FP32)   | **392 MB**                      |
| Weight Tying        | Yes (tok_emb â†” out_head)        |

---

## ğŸ“Š Training Details

### Infrastructure

| Component         | Details                                            |
| ----------------- | -------------------------------------------------- |
| GPU               | NVIDIA A100 (40 GB) via [Modal](https://modal.com) |
| Framework         | PyTorch 2.x                                        |
| Tokenizer         | tiktoken (GPT-2 BPE, 50,257 tokens)                |
| Training Platform | Modal (serverless GPU cloud)                       |

### Training Configuration

| Setting                 | Value                                     |
| ----------------------- | ----------------------------------------- |
| Batch Size              | 8,192 tokens (context_length Ã— batch)     |
| Total Training Tokens   | 269,098,817 (~269M)                       |
| Train/Val Split         | 90% / 10%                                 |
| Train Batches per Epoch | 29,564                                    |
| Optimizer               | AdamW (Î²1=0.9, Î²2=0.99, Îµ=1e-8, wd=0.1)   |
| Peak Learning Rate      | 4e-4                                      |
| Min Learning Rate       | 4e-5                                      |
| LR Schedule             | Linear warmup (2000 steps) â†’ Cosine decay |
| Gradient Clipping       | 1.0 (global norm)                         |
| Epochs                  | 5 (in progress)                           |

### Training Progress (Epoch 1 / 5)

| Metric           | Value                    |
| ---------------- | ------------------------ |
| Training Time    | 223 minutes (~3.7 hours) |
| Starting Loss    | 495.7                    |
| Final Train Loss | 2.760                    |
| Final Val Loss   | 2.674                    |
| Tokens Processed | 242M                     |
| Checkpoint Size  | 1,185 MB                 |

> Loss dropped from **495 â†’ 2.67** in a single epoch across 29,564 batches. Remaining 4 epochs will further improve quality.

### Fault-Tolerant Training Features

- **Per-epoch checkpointing** â€” Model + optimizer state saved after every epoch
- **Auto-download** â€” Checkpoints automatically downloaded from cloud to local machine
- **Resumable training** â€” Pass `--resume-from` flag to continue from any checkpoint
- **Per-epoch logs** â€” Training metrics saved as JSON after each epoch (no data loss on interruption)
- **Generator pattern** â€” `remote_gen()` streams results to client as epochs complete

---

## ğŸ“‚ Project Structure

```
LLM-FROM-SCRATCH/
â”‚
â”œâ”€â”€ llm_engine.py              # GPT model architecture (Transformer, MHA, FFN)
â”œâ”€â”€ data_loader.py             # Dataset/DataLoader (chunked tokenization)
â”œâ”€â”€ data_cleaner.py            # Raw legal text cleaning pipeline
â”œâ”€â”€ training.py                # Modal-based distributed training script
â”œâ”€â”€ count_params.py            # Parameter counting utility
â”‚
â”œâ”€â”€ inference/                 # Inference & web UI
â”‚   â”œâ”€â”€ infer.py               # Local inference engine (standalone or importable)
â”‚   â”œâ”€â”€ app.py                 # Flask web server
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ index.html         # Dark-themed web interface
â”‚
â”œâ”€â”€ checkpoints/               # Model checkpoints (not in git)
â”‚   â”œâ”€â”€ epoch_1_model_and_optimizer.pth
â”‚   â””â”€â”€ training_log_epoch_1.json
â”‚
â”œâ”€â”€ data/                      # Training corpus
â”‚   â””â”€â”€ combined_legal_data.txt
â”‚
â”œâ”€â”€ logs/                      # Raw training logs
â”‚   â””â”€â”€ epoch_1_logs.txt
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
git clone https://github.com/your-username/LLM-FROM-SCRATCH.git
cd LLM-FROM-SCRATCH

python -m venv .venv
.venv\Scripts\activate       # Windows
# source .venv/bin/activate  # Linux/Mac

pip install -r requirements.txt
```

### 2. Run Inference (CLI)

```bash
python inference/infer.py --prompt "The verdict of the court is " --max-tokens 200
```

### 3. Run Web Server

```bash
python inference/app.py
# Open http://localhost:5000
```

### 4. Train the Model (requires Modal account)

```bash
# First time - full training
modal run training.py --num-epochs 5 --eval-freq 50 --eval-iter 5

# Resume from checkpoint
modal run training.py --num-epochs 5 --resume-from runs/20260219-184305/epoch_1_model_and_optimizer.pth
```

---

## ğŸ› ï¸ Tech Stack

| Layer                   | Technology                  |
| ----------------------- | --------------------------- |
| Model Architecture      | Custom GPT (PyTorch)        |
| Tokenizer               | tiktoken (GPT-2 BPE)        |
| Training Infrastructure | Modal (serverless A100 GPU) |
| Inference               | PyTorch (CPU)               |
| Web Backend             | Flask                       |
| Web Frontend            | Vanilla HTML/CSS/JS         |
| Data                    | 1.25 GB Indian legal corpus |

---

## ğŸ“ˆ Sample Output

**Prompt:** `Under Section 498A of the Indian Penal Code,`

**Generated (Epoch 1):**

> Under Section 498A of the Indian Penal Code, as per the document, the charge under Section 376 IPC was based on the complaint, which was filed by the appellant before the trial court.

> _Note: After epoch 1, the model generates coherent legal English but may reference incorrect sections. Quality improves significantly with continued training (epochs 2-5)._

---

## ğŸ”® Roadmap

- [x] Custom GPT architecture from scratch
- [x] 103M parameter model training
- [x] Per-epoch checkpointing & resumable training
- [x] Local CPU inference engine
- [x] Web interface with generation controls
- [ ] Complete 5-epoch training
- [ ] RAG integration for grounded legal answers
- [ ] Fine-tuning for instruction-following
- [ ] Deployment to production

---

## ğŸ‘¨â€ğŸ’» Author

**Ashish Raj**

Built as a proof-of-concept for AI-powered legal intelligence in India.

---

## ğŸ“„ License

This project is for educational and research purposes. The training data consists of publicly available Indian court judgments.
